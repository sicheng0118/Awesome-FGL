# Notes on [ 2017 AISTATS ] FedAvg

⭐【基本信息】

<div align=center>
<img src="/Attachments/2017-AISTATS-FedAvg/Info.png">
</div>

---

⭐【速读】

<div align=center>
<img src="/Attachments/2017-AISTATS-FedAvg/KeyPoint.png" width="50%">
</div>

***<p align="center">FedAvg算法流程</p>*** 

---

⭐【摘要】

- Background
  - The performance of the model heavily relies on **the amount of training data**. However, those data are often **privacy-sensitive, large in quantity**, or both, which **precludes logging into the data center and training the model using conventional approaches**.
  
- Contributions
  - We propose to **leave the training data distributed**, and **learn a shared model by aggregating locally-computed updates**.

⭐【一句话总结】

这篇论文提出了一种名为联邦学习的通信高效方法，通过在不共享原始数据的情况下在移动设备上分布式训练深度神经网络，以保护用户隐私并减少通信成本。

**Q1: 本文的动机/试图解决的问题是什么？**

A：这篇论文试图解决的主要问题是如何在保护用户隐私的同时，利用移动设备上的大量分散数据来训练高质量的深度学习模型。具体来说，论文关注的挑战包括：

1. 隐私保护：现代移动设备能够访问大量可能包含敏感信息的数据，如位置数据、照片、语音和文本等。传统的数据收集和集中式训练方法可能会暴露这些敏感信息，增加隐私泄露的风险。

2. 数据量和通信成本：移动设备上的数据通常是大量且隐私敏感的，这使得将所有数据传输到中心服务器进行训练变得不可行，因为这样做不仅会消耗大量的存储空间，还会产生高昂的通信成本。

3. 非独立同分布（non-IID）数据：移动设备上的数据分布往往是非独立同分布的，即不同用户的本地数据集可能在分布上存在显著差异，这给模型的训练带来了额外的挑战。

为了解决这些问题，论文提出了一种名为联邦学习（Federated Learning）的方法，该方法允许多个设备（客户端）在不共享它们的原始数据的情况下，共同训练一个共享的模型。每个客户端在本地计算模型更新，然后将这些更新发送到服务器进行聚合，以此来更新全局模型。这种方法减少了对原始数据的访问需求，降低了隐私泄露的风险，并减少了通信成本。论文通过提出联邦平均（FederatedAveraging）算法，并对其进行了广泛的实证评估，证明了这种方法在多种模型架构和数据集上的有效性和鲁棒性。

**Q2: 有哪些相关研究？**

在论文中，作者提到了几项与联邦学习相关的研究领域和具体工作，这些研究为联邦学习的概念提供了理论基础和技术背景。以下是一些论文中提及的相关研究：

1. 分布式训练和迭代模型平均：McDonald等人[28]和Povey等人[31]研究了通过迭代平均本地训练模型来进行分布式训练的方法，特别是在感知机和深度神经网络（DNN）用于语音识别的情况下。Zhang等人[42]研究了一种异步方法，使用“软”平均。这些工作主要考虑集群/数据中心设置，并不考虑非IID和数据不平衡的特性，这些特性对于联邦学习环境至关重要。

2. 差分隐私：Abadi等人[1]探讨了如何在深度学习中使用差分隐私来保护用户数据的隐私。

3. 安全多方计算：这些技术允许在不泄露各自输入的情况下进行跨多个方的计算[18]。Bonawasser等人[6]介绍了一种针对联邦学习的高效安全聚合协议。

4. 通信高效的分布式优化：在凸优化和估计问题中，分布式优化和估计问题已经得到了广泛关注[4, 15, 33]。一些算法特别关注通信效率[45, 34, 40, 27, 43]。这些工作通常假设数据是IID分布的，并且每个节点有相同数量的数据点，这些假设在联邦优化设置中不成立。

5. 非凸神经网络目标：论文中考虑的算法适用于任何有限和目标，例如机器学习问题中的损失函数。

6. 大规模分布式SGD：Dean等人[12]应用分布式随机梯度下降（SGD）来训练神经网络，但这些方法在联邦设置中需要大量的更新。

7. 分布式共识算法：例如[41]，这些算法放宽了IID假设，但仍然不适用于通信受限的大规模客户端优化。

8. 一拍即合平均：这是考虑的算法家族的一个端点，其中每个客户端解决其本地数据上的模型，然后这些模型被平均以产生最终的全局模型。

这些相关研究表明，联邦学习领域建立在广泛的研究基础之上，涉及隐私保护、分布式计算、优化理论和深度学习等多个领域。论文的贡献在于将这些领域的技术结合起来，提出了一种适用于移动设备上分散数据的联邦学习方法。
